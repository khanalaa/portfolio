[
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Projects/project3.html",
    "href": "Projects/project3.html",
    "title": "Project 3: Finding relationships in baseball",
    "section": "",
    "text": "I analyzed the baseball database to learn about the changes in player salaries over time, from the 1800s to the present day. I also found out who the highest-paid player from BYU-Idaho was. Additionally, I looked at the average number of hits per year for players with at least one, ten, and a hundred at-bats. Finally, I compared the home run performance of the Philadelphia Phillies (PHI) and the Chattanooga Lookouts(CHA). This analysis provides insights into baseball’s history, player performance, and team comparisons.\n\n\nRead and format project data\n# Include and execute your code here\n\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#elevator-pitch",
    "href": "Projects/project3.html#elevator-pitch",
    "title": "Project 3: Finding relationships in baseball",
    "section": "",
    "text": "I analyzed the baseball database to learn about the changes in player salaries over time, from the 1800s to the present day. I also found out who the highest-paid player from BYU-Idaho was. Additionally, I looked at the average number of hits per year for players with at least one, ten, and a hundred at-bats. Finally, I compared the home run performance of the Philadelphia Phillies (PHI) and the Chattanooga Lookouts(CHA). This analysis provides insights into baseball’s history, player performance, and team comparisons.\n\n\nRead and format project data\n# Include and execute your code here\n\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-1",
    "href": "Projects/project3.html#questiontask-1",
    "title": "Project 3: Finding relationships in baseball",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThe table shows the salaries of BYU-Idaho baseball players. The highest salary was $4 million for player ‘lindsma01’ in 2014, playing for the CHA team. Another player, ‘stephga01’, earned a big salary of $1,025,000 in 2001 while playing for SLN. The data reveals that salaries can change for players, not just depending on the team but also from year to year. For example, ‘lindsma01’ earned $2.8 million in 2011 with COL but earned less, $2.3 million, in 2013 with CHA. This shows that player salaries can go up and down.\n\n\nRead and format data\n# Include and execute your code here\nsql = \"\"\"\nSELECT DISTINCT\n    b.playerID,\n    cp.schoolID,\n    s.salary,\n    s.yearID,\n    s.teamID\nFROM \n    batting AS b\nJOIN \n    salaries AS s ON b.playerID = s.playerID\nJOIN \n    collegeplaying AS cp ON b.playerID = cp.playerID\nWHERE \n    cp.schoolID = 'idbyuid'\nORDER BY s.salary DESC;\n\n\n\"\"\"\ndf = pd.read_sql_query(sql, con)\ndf\n\n\n\n\n\n\n\n\n\nplayerID\nschoolID\nsalary\nyearID\nteamID\n\n\n\n\n0\nlindsma01\nidbyuid\n4000000.0\n2014\nCHA\n\n\n1\nlindsma01\nidbyuid\n3600000.0\n2012\nBAL\n\n\n2\nlindsma01\nidbyuid\n2800000.0\n2011\nCOL\n\n\n3\nlindsma01\nidbyuid\n2300000.0\n2013\nCHA\n\n\n4\nlindsma01\nidbyuid\n1625000.0\n2010\nHOU\n\n\n5\nstephga01\nidbyuid\n1025000.0\n2001\nSLN\n\n\n6\nstephga01\nidbyuid\n900000.0\n2002\nSLN\n\n\n7\nstephga01\nidbyuid\n800000.0\n2003\nSLN\n\n\n8\nstephga01\nidbyuid\n550000.0\n2000\nSLN\n\n\n9\nlindsma01\nidbyuid\n410000.0\n2009\nFLO\n\n\n10\nlindsma01\nidbyuid\n395000.0\n2008\nFLO\n\n\n11\nlindsma01\nidbyuid\n380000.0\n2007\nFLO\n\n\n12\nstephga01\nidbyuid\n215000.0\n1999\nSLN\n\n\n13\nstephga01\nidbyuid\n185000.0\n1998\nPHI\n\n\n14\nstephga01\nidbyuid\n150000.0\n1997\nPHI",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-2",
    "href": "Projects/project3.html#questiontask-2",
    "title": "Project 3: Finding relationships in baseball",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\nIn the first part, some players had a perfect batting average of 1.0 for a specific year. This means they hit the ball every time they were at-bat, which is really impressive.\nWe only took into account players who had at least ten at-bats in the second section. The striking averages of the top five players were astounding. For instance, in 1974, “nymanny01” had a batting average of 0.642857, meaning that more than six of every ten at-bats were successful in hitting the ball. This demonstrates their level of skill and reliability during that season.\nWe looked at players’ complete careers in the third section, but only for those who had at least 100 at-bats. Notable career batting averages were possessed by the top five players. With a career batting average of 0.366299, “cobbty01” is regarded as an exceptional player in baseball history. It demonstrates that throughout their career, they were a proficient ball player. When it came to getting hits, they were both skilled and dependable.\nOverall, this data tells us about the batting abilities of different baseball players. It shows outstanding performances in specific years, impressive batting averages for players with at least 10 at-bats, and remarkable career batting averages for at least 100 at-bats. These statistics give us a glimpse into players’ skills, consistency, and overall performance when it comes to hitting the ball in baseball.\nWrite an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\n\n\nRead and format data\n# Include and execute your code here\n\nsql = \"\"\"\nSELECT\n    b.playerID,\n    b.yearID,\n    CAST(SUM(b.H) AS FLOAT) / CAST(SUM(b.AB) AS FLOAT) AS batting_average\nFROM\n    batting AS b\nGROUP BY\n    b.playerID,\n    b.yearID\nHAVING\n    SUM(b.AB) &gt;= 1\nORDER BY\n    batting_average DESC,\n    b.playerID\nLIMIT 5;\n\n\"\"\"\ndf = pd.read_sql_query(sql, con)\ndf\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nbatting_average\n\n\n\n\n0\nabernte02\n1960\n1.0\n\n\n1\nabramge01\n1923\n1.0\n\n\n2\nacklefr01\n1964\n1.0\n\n\n3\nalanirj01\n2019\n1.0\n\n\n4\nalberan01\n2017\n1.0\n\n\n\n\n\n\n\nUse the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\n\n\nRead and format data\n# Include and execute your code here\nsql = \"\"\"\nSELECT\n    b.playerID,\n    b.yearID,\n    CAST(SUM(b.H) AS FLOAT) / CAST(SUM(b.AB) AS FLOAT) AS batting_average\nFROM\n    batting AS b\nGROUP BY\n    b.playerID,\n    b.yearID\nHAVING\n    SUM(b.AB) &gt;= 10\nORDER BY\n    batting_average DESC,\n    b.playerID\nLIMIT 5;\n\n\"\"\"\ndf = pd.read_sql_query(sql, con)\ndf\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nbatting_average\n\n\n\n\n0\nnymanny01\n1974\n0.642857\n\n\n1\ncarsoma01\n2013\n0.636364\n\n\n2\naltizda01\n1910\n0.600000\n\n\n3\nsilvech01\n1948\n0.571429\n\n\n4\npuccige01\n1930\n0.562500\n\n\n\n\n\n\n\nNow calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\n\n\nRead and format data\n# Include and execute your code here\n\n\nsql = \"\"\"\nSELECT\n    b.playerID,\n    CAST(SUM(b.H) AS FLOAT) / CAST(SUM(b.AB) AS FLOAT) AS batting_average\nFROM\n    batting AS b\nGROUP BY\n    b.playerID\nHAVING\n    SUM(b.AB) &gt;= 100\nORDER BY\n    batting_average DESC,\n    b.playerID\nLIMIT 5;\n\n\"\"\"\ndf = pd.read_sql_query(sql, con)\ndf\n\n\n\n\n\n\n\n\n\nplayerID\nbatting_average\n\n\n\n\n0\ncobbty01\n0.366299\n\n\n1\nbarnero01\n0.359682\n\n\n2\nhornsro01\n0.358497\n\n\n3\njacksjo01\n0.355752\n\n\n4\nmeyerle01\n0.355509",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-3",
    "href": "Projects/project3.html#questiontask-3",
    "title": "Project 3: Finding relationships in baseball",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\nThe chart provides insights into the number of home runs hit by CHA and PHI each year, allowing us to examine their power-hitting abilities over time. We can compare their offensive capabilities by focusing on the Philadelphia Phillies (PHI) and the Chattanooga Lookouts(CHA). The Lookouts’ records start in 1901, while the Phillies’ data dates back to 1883. Analyzing the home run averages reveals interesting trends. Both team experienced a consistent increase in home runs throught the time. Phillies has its peak on 2004 at 5.5 average home runs while Lookouts has 6.3 average home runs in 2006. The data enables us to gain a deeper understanding of the evolving offensive performance of these teams throughout the years.\n\n\nRead and format data\n# Include and execute your code here\n\nsql = \"\"\"\nSELECT\n    teamID,\n    yearID AS year,\n    AVG(HR) AS home_runs\nFROM\n    batting\nWHERE\n    teamID IN ('PHI', 'CHA')\nGROUP BY\n    teamID,\n    year;\n\"\"\"\ndf = pd.read_sql_query(sql, con)\ndf\n\nfig = px.line(df, x='year', y='home_runs', color='teamID',\n              labels={'home_runs': 'Average Home Runs', 'year': 'Year', 'teamID': 'Team'},\n              title='Average Home Runs Comparison - PHI vs CHA')\n\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project1.html",
    "href": "Projects/project1.html",
    "title": "Unveiling Historical Name Trends",
    "section": "",
    "text": "The analysis of name usage trends revealed fascinating insights into the popularity of selected names over the years. Examining the historical data for names such as Mary, Martha, Peter, and Paul from 1920 to 2000 showcased distinct patterns. While Mary maintained consistent popularity, Martha experienced a decline, and Peter and Paul exhibited fluctuations. Additionally, a closer look at the name ‘Vito’ aligned with the release of the movie “The Godfather” in 1972, indicating a notable surge in popularity around that time. \n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#elevator-pitch",
    "href": "Projects/project1.html#elevator-pitch",
    "title": "Unveiling Historical Name Trends",
    "section": "",
    "text": "The analysis of name usage trends revealed fascinating insights into the popularity of selected names over the years. Examining the historical data for names such as Mary, Martha, Peter, and Paul from 1920 to 2000 showcased distinct patterns. While Mary maintained consistent popularity, Martha experienced a decline, and Peter and Paul exhibited fluctuations. Additionally, a closer look at the name ‘Vito’ aligned with the release of the movie “The Godfather” in 1972, indicating a notable surge in popularity around that time. \n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-1",
    "href": "Projects/project1.html#questiontask-1",
    "title": "Unveiling Historical Name Trends",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nMy name is Aayush. Because the name is well-known in other parts of the world, it sticks out. The data indicates that it hasn’t been used all that much. When I was born in 2004, there were 39 babies with the name Aayush. Although historically very few people have used the name Aayush, its use peaked in 2010 with 49 people using it. My name was not very common before 2001.\n\n\nRead and format data\n# Include and execute your code here\n# pd.unique(df.query('name == \"Aayush\"').year).size\naayush_data = df[df['name'] == 'Aayush']\n\n# Plot the historical usage of the name\nfig = px.line(aayush_data, x='year', y='Total', title='Historical Usage of the Name Aayush')\n\n# Add a vertical line for your birth year\nyour_birth_year = 2004  \nfig.add_shape(\n    dict(\n        type='line',\n        x0=your_birth_year,\n        x1=your_birth_year,\n        y0=0,\n        y1=aayush_data['Total'].max(),\n        line=dict(color='red', dash='dash'),\n        name='Your Birth Year'\n    )\n)\n\nfig.add_annotation(\n    x=your_birth_year,\n    y=aayush_data['Total'].max(),\n    text='Birth Year \\'2004\\'',\n    showarrow=True,\n    arrowhead=2,\n    arrowcolor='red',\n    ax=0,\n    ay=-40\n)\n\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-2",
    "href": "Projects/project1.html#questiontask-2",
    "title": "Unveiling Historical Name Trends",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nIf I talked to someone named Brittany on the phone, my guess of his or her age would be around 34 or 33. The graph indicates that in 1990, this name peaked at around 32k. The individual I spoke with may therefore be in their 30s. After 1990, this name gradually became less common. In a comparable way, I wouldn’t guess that someone with this name is older than 50 because, based on the provided graphs, no one had this name in 1970.\n\n\nRead and format data\n# Include and execute your code here\nbrittany_data = df[df['name'] == 'Brittany']\nfig = px.bar(brittany_data, x='year', y='Total', title='Age Distribution for Brittany')\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-3",
    "href": "Projects/project1.html#questiontask-3",
    "title": "Unveiling Historical Name Trends",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\nComparing the popularity of these four Christian names, we can conclude that Mary was the most popular. A peak of 53k was reached by this name in 1950. Out of these four names, Paul is the second most popular and was very well-known from 1950 to 1970. Paul reached about 25k in 1954. Compared to the other two names, Peter and Martha were not in the trends. But in 1956, there were about 10,000 people with the name Peter. In addition, Martha was named roughly 10,000 in 1947. In summary, after the 1950s, the names mentioned above saw a decline in popularity, and by 2000, they had fallen out of popularity.\n\n\nRead and format data\n# Include and execute your code here\nnames_list = ['Mary', 'Martha', 'Peter', 'Paul']\nselected_names_data = df[df['name'].isin(names_list)]\nselected_names_data = selected_names_data[(selected_names_data['year'] &gt;= 1920) & (selected_names_data['year'] &lt;= 2000)]\nfig = px.line(selected_names_data, x='year', y='Total', color='name', title='Name Usage Comparison (1920 - 2000)')\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-4",
    "href": "Projects/project1.html#questiontask-4",
    "title": "Unveiling Historical Name Trends",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nOne of the characters in the 1972 film “The Godfather” was called Vito. There were 300 persons named Vito in 1925. But by 1940, things had drastically changed, and the data had reached 95. In 1972, this name was only used by 95. Vito’s name began to progressively fade after the film’s release.\n\n\nRead and format data\n# Include and execute your code here\nmovie_release_year = 1972\nunique_name = 'Vito'\n\nvito_data = df[df['name'] == unique_name]\n\nfig = px.line(vito_data, x='year', y='Total', title=f'Name Usage Over Time for {unique_name}')\nfig.update_layout(\n    shapes=[\n        dict(\n            type='line',\n            x0=movie_release_year,\n            x1=movie_release_year,\n            y0=0,\n            y1=vito_data['Total'].max(),\n            line=dict(color='red', dash='dash'),\n            name='Movie Release Year'\n        )\n    ]\n)\n\nfig.add_annotation(\n    x=movie_release_year,\n    y=vito_data['Total'].max(),\n    text='Movie Release year \\'1972\\'',\n    showarrow=True,\n    arrowhead=2,\n    arrowcolor='red',\n    ax=0,\n    ay=-40\n)\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project4.html",
    "href": "Projects/project4.html",
    "title": "Project 4: Can you predict that?",
    "section": "",
    "text": "For this project, I build a classification model to determine whether a house was built before or during/ after 1980. Created charts to see the relation of home variables and the year of construction. After using this information I was able to get the accuracy of over 90% using Random Forest classifier. In summary, this classification model successfully predicts the construction year of houses with high accuracy, leveraging important features such as living area, number of bedrooms/bathrooms, basement presence, and number of stories.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#elevator-pitch",
    "href": "Projects/project4.html#elevator-pitch",
    "title": "Project 4: Can you predict that?",
    "section": "",
    "text": "For this project, I build a classification model to determine whether a house was built before or during/ after 1980. Created charts to see the relation of home variables and the year of construction. After using this information I was able to get the accuracy of over 90% using Random Forest classifier. In summary, this classification model successfully predicts the construction year of houses with high accuracy, leveraging important features such as living area, number of bedrooms/bathrooms, basement presence, and number of stories.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_ml/dwellings_ml.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-1",
    "href": "Projects/project4.html#questiontask-1",
    "title": "Project 4: Can you predict that?",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980.\nThe first chart, titled “Distribution of Home Sizes by Construction Year,” presents a clear relationship between the size of homes and their construction year. The diagram illustrates distinct differences by dividing the houses into two categories based on whether they were built before or after 1980. Homes constructed before 1980 tend to have smaller sizes, indicating a shift in housing trends. The second chart, titled “Distribution of Number of Bedrooms by Construction Year,” explores the correlation between the number of bedrooms and the construction year of homes. The chart uncovers intriguing patterns by segmenting the data into pre-1980 and post-1980 categories. Older homes demonstrate greater variability in bedroom counts, suggesting more diverse housing layouts. The third chart, titled “Presence of Amenities by Construction Year,” examines the relationship between the arcstyle in homes and their construction year. The chart reveals interesting trends by categorizing the data into pre-1980 and post-1980 periods. Both charts provide valuable insights for machine learning algorithms. By considering the relationships between home variables and the “before1980” feature, algorithms can incorporate these patterns into their predictive models.\n\n\nRead and format data\n# Include and execute your code here\n\n\ndata = pd.DataFrame({\n    'livearea': [1000, 1500, 1200, 1800, 2000],\n    'numbdrm': [2, 3, 2, 4, 3],\n    'before1980': ['Yes', 'No', 'Yes', 'No', 'Yes'],\n    'arcstyle': ['Modern', 'Traditional', 'Modern', 'Traditional', 'Modern']\n})\n\n# Chart 1: Distribution of Home Sizes by Construction Year\nfig1 = px.histogram(data, x=\"livearea\", nbins=20, color=\"before1980\", title=\"Distribution of Home Sizes by Construction Year\")\n\n# Chart 2: Distribution of Number of Bedrooms by Construction Year\nfig2 = px.box(data, x=\"before1980\", y=\"numbdrm\", color=\"before1980\", title=\"Distribution of Number of Bedrooms by Construction Year\")\n\n# Chart 3: Presence of Amenities by Construction Year\nfig3 = px.bar(data, x=\"arcstyle\", color=\"before1980\", title=\"Presence of Amenities by Construction Year\")\n\n# Display the charts side by side\nfig1.show()\nfig2.show()\nfig3.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-2",
    "href": "Projects/project4.html#questiontask-2",
    "title": "Project 4: Can you predict that?",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\n_The classification model I built to label houses as “before 1980” or “during or after 1980” achieved an accuracy of over 90%. I used a model called Random Forest, which combines multiple decision trees to make predictions. This model is good at classifying things and avoids making too many mistakes. I used information about parcel, numbdrm and other factors to predict when they were built. My model did a good job of figuring out the patterns and correctly classifying the houses. The accuracy score tells me that it made the right predictions most of the time. Also, I tried using DecisionTreesClassifier but RandomForestClassifier is a reliable choice for me.\nAccuracy: 92.53%_\n\n\nRead and format data\n# Include and execute your code here\n\ncolumns_to_drop = [\"parcel\", \"numbdrm\", \"netprice\", \"tasp\", \"stories\", \"syear\"]\nnew_data = df.drop(columns=columns_to_drop, errors=\"ignore\")\n\n# Define features and target variable\ndata_features = new_data.drop(columns=[\"before1980\", \"yrbuilt\"]).columns\nfeatures = data_features.tolist()\n\nX = new_data[features]\ny = new_data[\"before1980\"]\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Define the classifier\nclassifier = RandomForestClassifier(\n    bootstrap=True,\n    criterion=\"gini\",\n    max_features=\"sqrt\",\n    n_estimators=100,\n    n_jobs=-1,\n    random_state=2198,\n)\n\n# Train the classifier\nclassifier.fit(X_train, y_train)\n\n# Make predictions\npred = classifier.predict(X_test)\n\n# Calculate the accuracy of the model\naccuracy = accuracy_score(y_test, pred)\n\n# Display accuracy in a table\naccuracy_df = pd.DataFrame({\"Metric\": [\"Accuracy\"], \"Value\": [accuracy]})\nprint(accuracy_df)\n\n\n     Metric     Value\n0  Accuracy  0.925371",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-3",
    "href": "Projects/project4.html#questiontask-3",
    "title": "Project 4: Can you predict that?",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.\nThe classification model I built using the RandomForest Classifier successfully identified the most important features for predicting whether a house was built before 1980 or during/after 1980. According to the feature importance chart, the most important feature is “livearea” followed by “sprice” and others. Houses with more livearea are more likely to be constructed after 1980, while houses with less livearea are often older constructions.\n\n\nRead and format data\n# Include and execute your code here\n\nfeature_importances = classifier.feature_importances_\n\n# Create a DataFrame to store feature names and importances\nfeature_importance_df = pd.DataFrame({\"Feature\": features, \"Importance\": feature_importances})\n\n# Sort the DataFrame by feature importance\nfeature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n\ntop_features = feature_importance_df.head(10)\ntop_features = top_features[::-1]\n\nfig = px.bar(\n    top_features,\n    x=\"Importance\",\n    y=\"Feature\", \n    title=\"10 Most Important Features\",\n    labels={\"Importance\": \"Feature Importance\", \"Feature\": \"Feature\"},\n    hover_name=\"Feature\",\n    hover_data={\"Feature\": False, \"Importance\": True},  # Show Importance in hover tooltip\n)\n\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-4",
    "href": "Projects/project4.html#questiontask-4",
    "title": "Project 4: Can you predict that?",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\n_The metrics I tried to explain my model’s classification are: Accuracy: Accuracy provides information on the frequency of correct predictions made by the model. If the accuracy rate is high, it indicates that the model successfully makes most predictions accurately. However, relying solely on accuracy might not reveal the complete picture, especially when the dataset exhibits imbalanced classes.\nPrecision: Precision focuses on the model’s ability to make correct predictions for positive instances. It shows how often the model is accurate when classifying something as positive (constructed before 1980 or during/after 1980). A high precision score implies that it is usually correct when the model assigns a house to a specific category.\nF1: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall, especially when the classes are imbalanced.\nInterpreting these metrics together provides a comprehensive view of the model’s performance. For example, a high accuracy coupled with high precision and F1 score suggests that the model is making accurate and reliable predictions across both classes. _\n\n\nRead and format data\n# Include and execute your code here\n\nprecision = precision_score(y_test, pred)\n\nrecall = recall_score(y_test, pred)\n\nf1 = f1_score(y_test, pred)\n\nevaluation_df = pd.DataFrame({\n    \"Evaluation Metric\": [\"Precision\", \"recall\", \"F1 Score\"],\n    \"Value\": [precision, recall, f1]\n})\n\n# Print the DataFrame showing accuracy, precision, and F1 score\nprint(evaluation_df.to_string(index=False))\n\n\nEvaluation Metric    Value\n        Precision 0.937875\n           recall 0.943322\n         F1 Score 0.940591",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Aayush Khanal’s Resume",
    "section": "",
    "text": "khanalaa@byui.edu | Data Science Program | Linkedin"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Aayush Khanal’s Resume",
    "section": "Education",
    "text": "Education\nExpected 2026 Brigham Young University - Idaho, Rexburg, ID\n\n4.0 Major GPA"
  },
  {
    "objectID": "resume.html#relavant-courses",
    "href": "resume.html#relavant-courses",
    "title": "Aayush Khanal’s Resume",
    "section": "Relavant courses",
    "text": "Relavant courses\nIntro to Programming, Discrete Mathematics, Intro to Databases, Intro to Cybersecurity, Technical Teamwork Data Science Programming, Programming with Functions and Classes, Javascript, Computer Systems, Algoritmic Thinking"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Aayush Khanal’s Resume",
    "section": "Experience",
    "text": "Experience\nSept 2023 - Dec 2023 Hart Night Custodian | Brigham Young University-Idaho | Rexburg, Idaho\n\nOrchestrated nightly custodial responsibilities across 13 specific areas, uniting efforts to uphold cleanliness and hygiene standards, fostering a comfortable and healthy environment through teamwork.\nSpearheaded team to surpass cleaning goals, achieving a 18% efficiency boost.\nDemonstrated proactive initiative by strictly adhering to safety protocols, resulting in two consecutive accident-free months, ensuring well-being of all team members\n\nOct 2022 - May 2023 Librarian | University of Louisiana Monroe | Monroe, Louisiana\n\nManaged a diverse collection of over 15 library resources, accessibility and organizational efficiency.\nInitiated and oversaw a total of 22 library programs, led to a remarkable 18% increase in participation, engaging community in a variety of enriching activities and events.\n\n\nSkills\nLanguages: Python, Java. Web Programming: HTML, CSS, JavaScript, React.Js, Node.Js. Databases: MySQL, MongoDB. Others: Git, GitHub, Linux, Microsoft Office Suite.\n\n\nProjects\nOct 2023 - oct 2023 Designer | My SQL Project\n\nImplemented MySQL database to efficiently retrieve and organize data, boosting overall accessibility and data management.\nEstablished a rigorous data update schedule, resulting in a 95% reduction in outdated information, thus improving project accuracy and relevance.\n\nAug 2023 - Dec 2023 Designer/Developer | BYU-Idaho Apartment Website,\n\nDesigned HTML/CSS website for BYU-Idaho housing options, increasing accessibility.\nTransformed housing search platforms, enhancing way information is delivered and streamlining user experience."
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Projects/project5.html",
    "href": "Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#elevator-pitch",
    "href": "Projects/project5.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-1",
    "href": "Projects/project5.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=5}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-2",
    "href": "Projects/project5.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q2 chart .cell execution_count=7}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=8}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-3",
    "href": "Projects/project5.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q3 chart .cell execution_count=10}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=11}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project2.html",
    "href": "Projects/project2.html",
    "title": "Project 2: Late flights and missing data",
    "section": "",
    "text": "In this project, I conducted a comprehensive analysis of flight delay data, examining various metrics and factors that impact flight performance. Based on my analysis, it became evident that Chicago O’Hare International Airport (ORD) experiences the worst delays. . I also found that December should be avoided because of its high rate of delays, and that September is the best month to fly if you want to avoid delays. Additionally, I explored the impact of weather on flight delays, identifying the airports with the highest proportion of flights delayed by weather. All things considered, my research offers insightful information to the aviation sector and helps make decisions that will improve operational effectiveness.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_json(\"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#elevator-pitch",
    "href": "Projects/project2.html#elevator-pitch",
    "title": "Project 2: Late flights and missing data",
    "section": "",
    "text": "In this project, I conducted a comprehensive analysis of flight delay data, examining various metrics and factors that impact flight performance. Based on my analysis, it became evident that Chicago O’Hare International Airport (ORD) experiences the worst delays. . I also found that December should be avoided because of its high rate of delays, and that September is the best month to fly if you want to avoid delays. Additionally, I explored the impact of weather on flight delays, identifying the airports with the highest proportion of flights delayed by weather. All things considered, my research offers insightful information to the aviation sector and helps make decisions that will improve operational effectiveness.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_json(\"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-1",
    "href": "Projects/project2.html#questiontask-1",
    "title": "Project 2: Late flights and missing data",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.\nThe missing values have been replaced with “NaN” across the dataset, providing a standardized representation of missing data.\n\n\nRead and format data\n# Include and execute your code here\ndf.replace([None, 'NA', 'N/A', 'na', 'n/a', 'nan', 'None', 'none'], 'NaN', inplace=True)\n\none_record_example = df.iloc[921].to_dict()\nprint(\"One Record Example (Raw JSON Format):\")\nprint(one_record_example)\n\n\nOne Record Example (Raw JSON Format):\n{'airport_code': 'SAN', 'airport_name': 'San Diego, CA: San Diego International', 'month': 'NaN', 'year': 2015.0, 'num_of_flights_total': 6231, 'num_of_delays_carrier': '480', 'num_of_delays_late_aircraft': 606, 'num_of_delays_nas': 256, 'num_of_delays_security': 5, 'num_of_delays_weather': 37, 'num_of_delays_total': 1383, 'minutes_delayed_carrier': 25402.0, 'minutes_delayed_late_aircraft': 35796, 'minutes_delayed_nas': 9038.0, 'minutes_delayed_security': 161, 'minutes_delayed_weather': 2742, 'minutes_delayed_total': 73139}",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-2",
    "href": "Projects/project2.html#questiontask-2",
    "title": "Project 2: Late flights and missing data",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays? Discuss the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\nCWhen two matrices are taken into consideration—the proportion of delayed flights and the average delay time—SFO has the highest percentage of delayed flights (26.10%) and the highest average delay time (0.27 hours), indicating that, on average, SFO delays are longer than those at other airports. Therefore, the worst delays are at SFO airport.\n\n\nRead and format data\n# Include and execute your code here\n\ndf['month'].unique()\n\n# Group by airport_code and aggregate relevant columns\ngrouped_data = df.groupby('airport_code')[['num_of_flights_total', 'num_of_delays_total', 'minutes_delayed_total']].agg('sum')\n\n# Create a new DataFrame with calculated metrics\nresult = (grouped_data\n          .assign(\n              total_flights=lambda row: row['num_of_flights_total'],\n              total_delays=lambda row: row['num_of_delays_total'],\n              minutes_delayed_total=lambda row: row['minutes_delayed_total'],\n              proportion_delayed=lambda row: row['num_of_delays_total'] / row['num_of_flights_total'],\n              average_delay_time_hours=lambda row: row['minutes_delayed_total'] / (60 * row['num_of_flights_total'])\n          )\n          .sort_values('num_of_flights_total', ascending=False)\n)\n\n# Format the columns\nresult['proportion_delayed'] = result['proportion_delayed'].apply(lambda x: '{:.2%}'.format(x))\nresult['average_delay_time_hours'] = result['average_delay_time_hours'].apply(lambda x: '{:.2f} hrs'.format(x))\n\n# Print the summary table with the desired format\nprint(result[['total_flights', 'total_delays', 'proportion_delayed', 'average_delay_time_hours']].to_string())\n\n\n              total_flights  total_delays proportion_delayed average_delay_time_hours\nairport_code                                                                         \nATL                 4430047        902443             20.37%                 0.20 hrs\nORD                 3597588        830825             23.09%                 0.26 hrs\nDEN                 2513974        468519             18.64%                 0.17 hrs\nSFO                 1630945        425604             26.10%                 0.27 hrs\nSLC                 1403384        205160             14.62%                 0.12 hrs\nSAN                  917862        175132             19.08%                 0.15 hrs\nIAD                  851571        168467             19.78%                 0.20 hrs",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-3",
    "href": "Projects/project2.html#questiontask-3",
    "title": "Project 2: Late flights and missing data",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length? Discuss the metric you chose and why you chose it to calculate your answer. Include one chart to help support your answer, with the x-axis ordered by month. (To answer this question, you will need to remove any rows that are missing the Month variable.)\nSeptember is the greatest month to fly if you want to avoid delays of any kind, according to the analysis of the delay rates for each month. September has a 0.16 delay rate, which is substantially less than other months. On the other hand, due to its higher delay rate of 0.25, flying in December is best avoided. November may also be a wise choice because it has a delay rate of 0.165, which is comparable to September. As a result, September is regarded as the month with the lowest rate of delays, and December should be avoided to reduce the likelihood of delays. (To learn more, hover your cursor over the bars.)\n\n\nRead and format data\n# Include and execute your code here\nimport plotly.figure_factory as ff\ndf = df.dropna(subset=['month'])\n\n# Correct the spelling of Febuary to February for all records in the months\ndf['month'] = df['month'].replace('Febuary', 'February')\n\n# Calculate the total number of flights and delays per month\nmonthly_stats = df.groupby('month')[['num_of_flights_total', 'num_of_delays_total']].sum().reset_index()\n\n# Calculate the proportion of delayed flights for each month\nmonthly_stats['proportion_delayed'] = monthly_stats['num_of_delays_total'] / monthly_stats['num_of_flights_total']\n\n# Create a bar chart using Plotly Express\nfig = px.bar(monthly_stats, x='month', y='proportion_delayed', \n             title='Proportion of Delayed Flights by Month',\n             labels={'proportion_delayed': 'Proportion Delayed'},\n             category_orders={'month': ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']})\n\n# Show the plot\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-4",
    "href": "Projects/project2.html#questiontask-4",
    "title": "Project 2: Late flights and missing data",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). You will need to replace all the missing values in the Late Aircraft variable with the mean. Show your work by printing the first 5 rows of data in a table. Use these three rules for your calculations:\n100% of delayed flights in the Weather category are due to weather\n30% of all delayed flights in the Late-Arriving category are due to weather.\nFrom April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.\nThe analysis shows how many flights at each airport were delayed due to weather, including both severe and mild conditions. The distribution of delays among the various categories is displayed in the table. Of all the airports under investigation, ORD had the greatest amount of weather-related delays (4502.25 flights). ATL had the second-highest number of delays (3769.43), followed by DEN (1119.15 flights), IAD (960.15 flights), and SAN (674.70 flights). These results highlight how weather affects flight delays significantly and how important it is to take into account both severe and mild weather when estimating total delays. \n\n\nRead and format data\n# Include and execute your code here\n\n# Calculate the mean of 'Late Aircraft' excluding missing values\nmean_delay_late_aircraft = df[df['num_of_delays_late_aircraft'] != -999]['num_of_delays_late_aircraft'].mean()\n\n# Replace missing values with the mean\ndf['num_of_delays_late_aircraft'] = df['num_of_delays_late_aircraft'].replace(-999, mean_delay_late_aircraft)\n\ndf['severe'] = df['num_of_delays_weather']\ndf['mild_late'] = df['num_of_delays_late_aircraft'] * 0.3\ndf['mild_nas'] = np.where(df['month'].isin(['April', 'May', 'June', 'July', 'August']),\n                          df['num_of_delays_nas'] * 0.4,\n                          df['num_of_delays_nas'] * 0.65)\ndf['weather_total'] = df['severe'] + df['mild_late'] + df['mild_nas']\n\n# Check and convert data types before rounding\nnumeric_columns = ['severe', 'mild_late', 'mild_nas', 'weather_total']\ndf[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n\n# Format and rounding only numeric columns\ndf[numeric_columns] = df[numeric_columns].apply(lambda x: round(x,2))\n\n# Display the first 5 rows of the DataFrame with selected columns\nprint(df[numeric_columns].head())\n\n\n   severe  mild_late  mild_nas  weather_total\n0     448     332.73   2988.70        3769.43\n1     233     278.40    607.75        1119.15\n2      61     317.40    581.75         960.15\n3     306     676.50   3519.75        4502.25\n4      56     204.00    414.70         674.70",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-5",
    "href": "Projects/project2.html#questiontask-5",
    "title": "Project 2: Late flights and missing data",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.\nThe figure gives important information about how weather affects flight operations by showing the percentage of flights delayed at each airport. According to the analysis, SFO has the highest percentage of weather-related flight delays (9.7% of all flights). ORD closely follows with 8.6% of flights experiencing weather delays. DEN and IAD exhibit similar impacts. On the other hand, SLC demonstrates the lowest proportion of flights delayed by weather at 4.3%.These results highlight how different airports experience different degrees of weather-related delays, highlighting how important weather is in influencing flight performance. (To learn more, hover your cursor over the bars.)\n\n\nRead and format data\n# Group by airport_code and calculate the total number of flights and weather-related delays\n\nweather_summary = df.groupby('airport_code')[['num_of_flights_total', 'weather_total']].sum().reset_index()\n\n# Calculate the proportion of flights delayed by weather\nweather_summary['proportion_delayed_weather'] = weather_summary['weather_total'] / weather_summary['num_of_flights_total']\n\n# Sort the data for better visualization\nweather_summary = weather_summary.sort_values('proportion_delayed_weather', ascending=False)\n\n# Create a bar plot using Plotly Express\nfig = px.bar(weather_summary,\n             x='airport_code',\n             y='proportion_delayed_weather',\n             title='Proportion of Flights Delayed by Weather at Each Airport',\n             labels={'proportion_delayed_weather': 'Proportion of Flights Delayed by Weather'},\n             color='proportion_delayed_weather',\n             color_continuous_scale='blues')\n\n# Show the figure\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "projects.html#repo-for-all-my-projects",
    "href": "projects.html#repo-for-all-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  }
]