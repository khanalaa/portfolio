[
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Projects/project3.html",
    "href": "Projects/project3.html",
    "title": "Project 3: Finding relationships in baseball",
    "section": "",
    "text": "I analyzed the baseball database to learn about the changes in player salaries over time, from the 1800s to the present day. I also found out who the highest-paid player from BYU-Idaho was. Additionally, I looked at the average number of hits per year for players with at least one, ten, and a hundred at-bats. Finally, I compared the home run performance of the Philadelphia Phillies (PHI) and the Chattanooga Lookouts(CHA). This analysis provides insights into baseball’s history, player performance, and team comparisons.\n\n\nRead and format project data\n# Include and execute your code here\n\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#elevator-pitch",
    "href": "Projects/project3.html#elevator-pitch",
    "title": "Project 3: Finding relationships in baseball",
    "section": "",
    "text": "I analyzed the baseball database to learn about the changes in player salaries over time, from the 1800s to the present day. I also found out who the highest-paid player from BYU-Idaho was. Additionally, I looked at the average number of hits per year for players with at least one, ten, and a hundred at-bats. Finally, I compared the home run performance of the Philadelphia Phillies (PHI) and the Chattanooga Lookouts(CHA). This analysis provides insights into baseball’s history, player performance, and team comparisons.\n\n\nRead and format project data\n# Include and execute your code here\n\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-1",
    "href": "Projects/project3.html#questiontask-1",
    "title": "Project 3: Finding relationships in baseball",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThe table shows the salaries of BYU-Idaho baseball players. The highest salary was $4 million for player ‘lindsma01’ in 2014, playing for the CHA team. Another player, ‘stephga01’, earned a big salary of $1,025,000 in 2001 while playing for SLN. The data reveals that salaries can change for players, not just depending on the team but also from year to year. For example, ‘lindsma01’ earned $2.8 million in 2011 with COL but earned less, $2.3 million, in 2013 with CHA. This shows that player salaries can go up and down.\n\n\nRead and format data\n# Include and execute your code here\nsql = \"\"\"\nSELECT DISTINCT\n    b.playerID,\n    cp.schoolID,\n    s.salary,\n    s.yearID,\n    s.teamID\nFROM \n    batting AS b\nJOIN \n    salaries AS s ON b.playerID = s.playerID\nJOIN \n    collegeplaying AS cp ON b.playerID = cp.playerID\nWHERE \n    cp.schoolID = 'idbyuid'\nORDER BY s.salary DESC;\n\n\n\"\"\"\ndf = pd.read_sql_query(sql, con)\ndf\n\n\n\n\n\n\n\n\n\nplayerID\nschoolID\nsalary\nyearID\nteamID\n\n\n\n\n0\nlindsma01\nidbyuid\n4000000.0\n2014\nCHA\n\n\n1\nlindsma01\nidbyuid\n3600000.0\n2012\nBAL\n\n\n2\nlindsma01\nidbyuid\n2800000.0\n2011\nCOL\n\n\n3\nlindsma01\nidbyuid\n2300000.0\n2013\nCHA\n\n\n4\nlindsma01\nidbyuid\n1625000.0\n2010\nHOU\n\n\n5\nstephga01\nidbyuid\n1025000.0\n2001\nSLN\n\n\n6\nstephga01\nidbyuid\n900000.0\n2002\nSLN\n\n\n7\nstephga01\nidbyuid\n800000.0\n2003\nSLN\n\n\n8\nstephga01\nidbyuid\n550000.0\n2000\nSLN\n\n\n9\nlindsma01\nidbyuid\n410000.0\n2009\nFLO\n\n\n10\nlindsma01\nidbyuid\n395000.0\n2008\nFLO\n\n\n11\nlindsma01\nidbyuid\n380000.0\n2007\nFLO\n\n\n12\nstephga01\nidbyuid\n215000.0\n1999\nSLN\n\n\n13\nstephga01\nidbyuid\n185000.0\n1998\nPHI\n\n\n14\nstephga01\nidbyuid\n150000.0\n1997\nPHI",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-2",
    "href": "Projects/project3.html#questiontask-2",
    "title": "Project 3: Finding relationships in baseball",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\nIn the first part, some players had a perfect batting average of 1.0 for a specific year. This means they hit the ball every time they were at-bat, which is really impressive.\nIn the second part, we only considered players who had at least 10 at-bats. The top five players had amazing batting averages. For example, ‘nymanny01’ had a batting average of 0.642857 in 1974, which means they successfully hit the ball more than 6 times out of every 10 at-bats. This shows how skilled and consistent they were during that season.\nIn the third part, we looked at players’ entire careers, but only those with at least 100 at-bats. The top five players had notable career batting averages. ‘cobbty01’ had a career batting average of 0.366299, considered exceptional in baseball history. It shows that they were consistently good at hitting the ball throughout their career. They were talented and reliable when it came to getting hits.\nOverall, this data tells us about the batting abilities of different baseball players. It shows outstanding performances in specific years, impressive batting averages for players with at least 10 at-bats, and remarkable career batting averages for at least 100 at-bats. These statistics give us a glimpse into players’ skills, consistency, and overall performance when it comes to hitting the ball in baseball.\nWrite an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\n\n\nRead and format data\n# Include and execute your code here\n\nsql = \"\"\"\nSELECT\n    b.playerID,\n    b.yearID,\n    CAST(SUM(b.H) AS FLOAT) / CAST(SUM(b.AB) AS FLOAT) AS batting_average\nFROM\n    batting AS b\nGROUP BY\n    b.playerID,\n    b.yearID\nHAVING\n    SUM(b.AB) &gt;= 1\nORDER BY\n    batting_average DESC,\n    b.playerID\nLIMIT 5;\n\n\"\"\"\ndf = pd.read_sql_query(sql, con)\ndf\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nbatting_average\n\n\n\n\n0\nabernte02\n1960\n1.0\n\n\n1\nabramge01\n1923\n1.0\n\n\n2\nacklefr01\n1964\n1.0\n\n\n3\nalanirj01\n2019\n1.0\n\n\n4\nalberan01\n2017\n1.0\n\n\n\n\n\n\n\nUse the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\n\n\nRead and format data\n# Include and execute your code here\nsql = \"\"\"\nSELECT\n    b.playerID,\n    b.yearID,\n    CAST(SUM(b.H) AS FLOAT) / CAST(SUM(b.AB) AS FLOAT) AS batting_average\nFROM\n    batting AS b\nGROUP BY\n    b.playerID,\n    b.yearID\nHAVING\n    SUM(b.AB) &gt;= 10\nORDER BY\n    batting_average DESC,\n    b.playerID\nLIMIT 5;\n\n\"\"\"\ndf = pd.read_sql_query(sql, con)\ndf\n\n\n\n\n\n\n\n\n\nplayerID\nyearID\nbatting_average\n\n\n\n\n0\nnymanny01\n1974\n0.642857\n\n\n1\ncarsoma01\n2013\n0.636364\n\n\n2\naltizda01\n1910\n0.600000\n\n\n3\nsilvech01\n1948\n0.571429\n\n\n4\npuccige01\n1930\n0.562500\n\n\n\n\n\n\n\nNow calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\n\n\nRead and format data\n# Include and execute your code here\n\n\nsql = \"\"\"\nSELECT\n    b.playerID,\n    CAST(SUM(b.H) AS FLOAT) / CAST(SUM(b.AB) AS FLOAT) AS batting_average\nFROM\n    batting AS b\nGROUP BY\n    b.playerID\nHAVING\n    SUM(b.AB) &gt;= 100\nORDER BY\n    batting_average DESC,\n    b.playerID\nLIMIT 5;\n\n\"\"\"\ndf = pd.read_sql_query(sql, con)\ndf\n\n\n\n\n\n\n\n\n\nplayerID\nbatting_average\n\n\n\n\n0\ncobbty01\n0.366299\n\n\n1\nbarnero01\n0.359682\n\n\n2\nhornsro01\n0.358497\n\n\n3\njacksjo01\n0.355752\n\n\n4\nmeyerle01\n0.355509",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-3",
    "href": "Projects/project3.html#questiontask-3",
    "title": "Project 3: Finding relationships in baseball",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\nThe chart provides insights into the number of home runs hit by CHA and PHI each year, allowing us to examine their power-hitting abilities over time. We can compare their offensive capabilities by focusing on the Philadelphia Phillies (PHI) and the Chattanooga Lookouts(CHA). The Lookouts’ records start in 1901, while the Phillies’ data dates back to 1883. Analyzing the home run averages reveals interesting trends. Both team experienced a consistent increase in home runs throught the time. Phillies has its peak on 2004 at 5.5 average home runs while Lookouts has 6.3 average home runs in 2006. The data enables us to gain a deeper understanding of the evolving offensive performance of these teams throughout the years.\n\n\nRead and format data\n# Include and execute your code here\n\nsql = \"\"\"\nSELECT\n    teamID,\n    yearID AS year,\n    AVG(HR) AS home_runs\nFROM\n    batting\nWHERE\n    teamID IN ('PHI', 'CHA')\nGROUP BY\n    teamID,\n    year;\n\"\"\"\ndf = pd.read_sql_query(sql, con)\ndf\n\nfig = px.line(df, x='year', y='home_runs', color='teamID',\n              labels={'home_runs': 'Average Home Runs', 'year': 'Year', 'teamID': 'Team'},\n              title='Average Home Runs Comparison - BAL vs DET')\n\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project1.html",
    "href": "Projects/project1.html",
    "title": "Unveiling Historical Name Trends",
    "section": "",
    "text": "The analysis of name usage trends revealed fascinating insights into the popularity of selected names over the years. Examining the historical data for names such as Mary, Martha, Peter, and Paul from 1920 to 2000 showcased distinct patterns. While Mary maintained consistent popularity, Martha experienced a decline, and Peter and Paul exhibited fluctuations. Additionally, a closer look at the name ‘Vito’ aligned with the release of the movie “The Godfather” in 1972, indicating a notable surge in popularity around that time. \n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#elevator-pitch",
    "href": "Projects/project1.html#elevator-pitch",
    "title": "Unveiling Historical Name Trends",
    "section": "",
    "text": "The analysis of name usage trends revealed fascinating insights into the popularity of selected names over the years. Examining the historical data for names such as Mary, Martha, Peter, and Paul from 1920 to 2000 showcased distinct patterns. While Mary maintained consistent popularity, Martha experienced a decline, and Peter and Paul exhibited fluctuations. Additionally, a closer look at the name ‘Vito’ aligned with the release of the movie “The Godfather” in 1972, indicating a notable surge in popularity around that time. \n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4names/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-1",
    "href": "Projects/project1.html#questiontask-1",
    "title": "Unveiling Historical Name Trends",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nMy name is Aayush. Because the name is well-known in other parts of the world, it sticks out. The data indicates that it hasn’t been used all that much. When I was born in 2004, there were 39 babies with the name Aayush. Although historically very few people have used the name Aayush, its use peaked in 2010 with 49 people using it. My name was not very common before 2001.\n\n\nRead and format data\n# Include and execute your code here\n# pd.unique(df.query('name == \"Aayush\"').year).size\naayush_data = df[df['name'] == 'Aayush']\n\n# Plot the historical usage of the name\nfig = px.line(aayush_data, x='year', y='Total', title='Historical Usage of the Name Aayush')\n\n# Add a vertical line for your birth year\nyour_birth_year = 2004  \nfig.add_shape(\n    dict(\n        type='line',\n        x0=your_birth_year,\n        x1=your_birth_year,\n        y0=0,\n        y1=aayush_data['Total'].max(),\n        line=dict(color='red', dash='dash'),\n        name='Your Birth Year'\n    )\n)\n\nfig.add_annotation(\n    x=your_birth_year,\n    y=aayush_data['Total'].max(),\n    text='Birth Year \\'2004\\'',\n    showarrow=True,\n    arrowhead=2,\n    arrowcolor='red',\n    ax=0,\n    ay=-40\n)\n\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-2",
    "href": "Projects/project1.html#questiontask-2",
    "title": "Unveiling Historical Name Trends",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nIf I talked to someone named Brittany on the phone, my guess of his or her age would be around 34 or 33. The graph indicates that in 1990, this name peaked at around 32k. The individual I spoke with may therefore be in their 30s. After 1990, this name gradually became less common. In a comparable way, I wouldn’t guess that someone with this name is older than 50 because, based on the provided graphs, no one had this name in 1970.\n\n\nRead and format data\n# Include and execute your code here\nbrittany_data = df[df['name'] == 'Brittany']\nfig = px.bar(brittany_data, x='year', y='Total', title='Age Distribution for Brittany')\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-3",
    "href": "Projects/project1.html#questiontask-3",
    "title": "Unveiling Historical Name Trends",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\nComparing the popularity of these four Christian names, we can conclude that Mary was the most popular. A peak of 53k was reached by this name in 1950. Out of these four names, Paul is the second most popular and was very well-known from 1950 to 1970. Paul reached about 25k in 1954. Compared to the other two names, Peter and Martha were not in the trends. But in 1956, there were about 10,000 people with the name Peter. In addition, Martha was named roughly 10,000 in 1947. In summary, after the 1950s, the names mentioned above saw a decline in popularity, and by 2000, they had fallen out of popularity.\n\n\nRead and format data\n# Include and execute your code here\nnames_list = ['Mary', 'Martha', 'Peter', 'Paul']\nselected_names_data = df[df['name'].isin(names_list)]\nselected_names_data = selected_names_data[(selected_names_data['year'] &gt;= 1920) & (selected_names_data['year'] &lt;= 2000)]\nfig = px.line(selected_names_data, x='year', y='Total', color='name', title='Name Usage Comparison (1920 - 2000)')\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-4",
    "href": "Projects/project1.html#questiontask-4",
    "title": "Unveiling Historical Name Trends",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\nOne of the characters in the 1972 film “The Godfather” was called Vito. There were 300 persons named Vito in 1925. But by 1940, things had drastically changed, and the data had reached 95. In 1972, this name was only used by 95. Vito’s name began to progressively fade after the film’s release.\n\n\nRead and format data\n# Include and execute your code here\nmovie_release_year = 1972\nunique_name = 'Vito'\n\nvito_data = df[df['name'] == unique_name]\n\nfig = px.line(vito_data, x='year', y='Total', title=f'Name Usage Over Time for {unique_name}')\nfig.update_layout(\n    shapes=[\n        dict(\n            type='line',\n            x0=movie_release_year,\n            x1=movie_release_year,\n            y0=0,\n            y1=vito_data['Total'].max(),\n            line=dict(color='red', dash='dash'),\n            name='Movie Release Year'\n        )\n    ]\n)\n\nfig.add_annotation(\n    x=movie_release_year,\n    y=vito_data['Total'].max(),\n    text='Movie Release year \\'1972\\'',\n    showarrow=True,\n    arrowhead=2,\n    arrowcolor='red',\n    ax=0,\n    ay=-40\n)\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project4.html",
    "href": "Projects/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#elevator-pitch",
    "href": "Projects/project4.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-1",
    "href": "Projects/project4.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=5}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-2",
    "href": "Projects/project4.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q2 chart .cell execution_count=7}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=8}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-3",
    "href": "Projects/project4.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q3 chart .cell execution_count=10}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=11}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Aayush Khanal’s Resume",
    "section": "",
    "text": "khanalaa@byui.edu | Data Science Program | Linkedin"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Aayush Khanal’s Resume",
    "section": "Education",
    "text": "Education\nExpected 2026 Brigham Young University - Idaho, Rexburg, ID\n\n4.0 Major GPA"
  },
  {
    "objectID": "resume.html#relavant-courses",
    "href": "resume.html#relavant-courses",
    "title": "Aayush Khanal’s Resume",
    "section": "Relavant courses",
    "text": "Relavant courses\nIntro to Programming, Discrete Mathematics, Intro to Databases, Intro to Cybersecurity, Technical Teamwork Data Science Programming, Programming with Functions and Classes, Javascript, Computer Systems, Algoritmic Thinking"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Aayush Khanal’s Resume",
    "section": "Experience",
    "text": "Experience\nSept 2023 - Dec 2023 Hart Night Custodian | Brigham Young University-Idaho | Rexburg, Idaho\n\nOrchestrated nightly custodial responsibilities across 13 specific areas, uniting efforts to uphold cleanliness and hygiene standards, fostering a comfortable and healthy environment through teamwork.\nSpearheaded team to surpass cleaning goals, achieving a 18% efficiency boost.\nDemonstrated proactive initiative by strictly adhering to safety protocols, resulting in two consecutive accident-free months, ensuring well-being of all team members\n\nOct 2022 - May 2023 Librarian | University of Louisiana Monroe | Monroe, Louisiana\n\nManaged a diverse collection of over 15 library resources, accessibility and organizational efficiency.\nInitiated and oversaw a total of 22 library programs, led to a remarkable 18% increase in participation, engaging community in a variety of enriching activities and events.\n\n\nSkills\nLanguages: Python, Java. Web Programming: HTML, CSS, JavaScript, React.Js, Node.Js. Databases: MySQL, MongoDB. Others: Git, GitHub, Linux, Microsoft Office Suite.\n\n\nProjects\nOct 2023 - oct 2023 Designer | My SQL Project\n\nImplemented MySQL database to efficiently retrieve and organize data, boosting overall accessibility and data management.\nEstablished a rigorous data update schedule, resulting in a 95% reduction in outdated information, thus improving project accuracy and relevance.\n\nAug 2023 - Dec 2023 Designer/Developer | BYU-Idaho Apartment Website,\n\nDesigned HTML/CSS website for BYU-Idaho housing options, increasing accessibility.\nTransformed housing search platforms, enhancing way information is delivered and streamlining user experience."
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Projects/project5.html",
    "href": "Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#elevator-pitch",
    "href": "Projects/project5.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-1",
    "href": "Projects/project5.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=5}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-2",
    "href": "Projects/project5.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q2 chart .cell execution_count=7}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=8}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-3",
    "href": "Projects/project5.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q3 chart .cell execution_count=10}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=11}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project2.html",
    "href": "Projects/project2.html",
    "title": "Project 2: Late flights and missing data",
    "section": "",
    "text": "In this project, I conducted a comprehensive analysis of flight delay data, examining various metrics and factors that impact flight performance. Based on my analysis, it became evident that Chicago O’Hare International Airport (ORD) experiences the worst delays. . I also found that December should be avoided because of its high rate of delays, and that September is the best month to fly if you want to avoid delays. Additionally, I explored the impact of weather on flight delays, identifying the airports with the highest proportion of flights delayed by weather. All things considered, my research offers insightful information to the aviation sector and helps make decisions that will improve operational effectiveness.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_json(\"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#elevator-pitch",
    "href": "Projects/project2.html#elevator-pitch",
    "title": "Project 2: Late flights and missing data",
    "section": "",
    "text": "In this project, I conducted a comprehensive analysis of flight delay data, examining various metrics and factors that impact flight performance. Based on my analysis, it became evident that Chicago O’Hare International Airport (ORD) experiences the worst delays. . I also found that December should be avoided because of its high rate of delays, and that September is the best month to fly if you want to avoid delays. Additionally, I explored the impact of weather on flight delays, identifying the airports with the highest proportion of flights delayed by weather. All things considered, my research offers insightful information to the aviation sector and helps make decisions that will improve operational effectiveness.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_json(\"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-1",
    "href": "Projects/project2.html#questiontask-1",
    "title": "Project 2: Late flights and missing data",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the “NaN” for at least one missing value.\nThe missing values have been replaced with “NaN” across the dataset, providing a standardized representation of missing data.\n\n\nRead and format data\n# Include and execute your code here\ndf.replace([None, 'NA', 'N/A', 'na', 'n/a', 'nan', 'None', 'none'], 'NaN', inplace=True)\n\none_record_example = df.iloc[921].to_dict()\nprint(\"One Record Example (Raw JSON Format):\")\nprint(one_record_example)\n\n\nOne Record Example (Raw JSON Format):\n{'airport_code': 'SAN', 'airport_name': 'San Diego, CA: San Diego International', 'month': 'NaN', 'year': 2015.0, 'num_of_flights_total': 6231, 'num_of_delays_carrier': '480', 'num_of_delays_late_aircraft': 606, 'num_of_delays_nas': 256, 'num_of_delays_security': 5, 'num_of_delays_weather': 37, 'num_of_delays_total': 1383, 'minutes_delayed_carrier': 25402.0, 'minutes_delayed_late_aircraft': 35796, 'minutes_delayed_nas': 9038.0, 'minutes_delayed_security': 161, 'minutes_delayed_weather': 2742, 'minutes_delayed_total': 73139}",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-2",
    "href": "Projects/project2.html#questiontask-2",
    "title": "Project 2: Late flights and missing data",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays? Discuss the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\nCWhen two matrices are taken into consideration—the proportion of delayed flights and the average delay time—SFO has the highest percentage of delayed flights (26.10%) and the highest average delay time (0.27 hours), indicating that, on average, SFO delays are longer than those at other airports. Therefore, the worst delays are at SFO airport.\n\n\nRead and format data\n# Include and execute your code here\n\ndf['month'].unique()\n\n# Group by airport_code and aggregate relevant columns\ngrouped_data = df.groupby('airport_code')[['num_of_flights_total', 'num_of_delays_total', 'minutes_delayed_total']].agg('sum')\n\n# Create a new DataFrame with calculated metrics\nresult = (grouped_data\n          .assign(\n              total_flights=lambda row: row['num_of_flights_total'],\n              total_delays=lambda row: row['num_of_delays_total'],\n              minutes_delayed_total=lambda row: row['minutes_delayed_total'],\n              proportion_delayed=lambda row: row['num_of_delays_total'] / row['num_of_flights_total'],\n              average_delay_time_hours=lambda row: row['minutes_delayed_total'] / (60 * row['num_of_flights_total'])\n          )\n          .sort_values('num_of_flights_total', ascending=False)\n)\n\n# Format the columns\nresult['proportion_delayed'] = result['proportion_delayed'].apply(lambda x: '{:.2%}'.format(x))\nresult['average_delay_time_hours'] = result['average_delay_time_hours'].apply(lambda x: '{:.2f} hrs'.format(x))\n\n# Print the summary table with the desired format\nprint(result[['total_flights', 'total_delays', 'proportion_delayed', 'average_delay_time_hours']].to_string())\n\n\n              total_flights  total_delays proportion_delayed average_delay_time_hours\nairport_code                                                                         \nATL                 4430047        902443             20.37%                 0.20 hrs\nORD                 3597588        830825             23.09%                 0.26 hrs\nDEN                 2513974        468519             18.64%                 0.17 hrs\nSFO                 1630945        425604             26.10%                 0.27 hrs\nSLC                 1403384        205160             14.62%                 0.12 hrs\nSAN                  917862        175132             19.08%                 0.15 hrs\nIAD                  851571        168467             19.78%                 0.20 hrs",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-3",
    "href": "Projects/project2.html#questiontask-3",
    "title": "Project 2: Late flights and missing data",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length? Discuss the metric you chose and why you chose it to calculate your answer. Include one chart to help support your answer, with the x-axis ordered by month. (To answer this question, you will need to remove any rows that are missing the Month variable.)\nSeptember is the greatest month to fly if you want to avoid delays of any kind, according to the analysis of the delay rates for each month. September has a 0.16 delay rate, which is substantially less than other months. On the other hand, due to its higher delay rate of 0.25, flying in December is best avoided. November may also be a wise choice because it has a delay rate of 0.165, which is comparable to September. As a result, September is regarded as the month with the lowest rate of delays, and December should be avoided to reduce the likelihood of delays. (To learn more, hover your cursor over the bars.)\n\n\nRead and format data\n# Include and execute your code here\nimport plotly.figure_factory as ff\ndf = df.dropna(subset=['month'])\n\n# Correct the spelling of Febuary to February for all records in the months\ndf['month'] = df['month'].replace('Febuary', 'February')\n\n# Calculate the total number of flights and delays per month\nmonthly_stats = df.groupby('month')[['num_of_flights_total', 'num_of_delays_total']].sum().reset_index()\n\n# Calculate the proportion of delayed flights for each month\nmonthly_stats['proportion_delayed'] = monthly_stats['num_of_delays_total'] / monthly_stats['num_of_flights_total']\n\n# Create a bar chart using Plotly Express\nfig = px.bar(monthly_stats, x='month', y='proportion_delayed', \n             title='Proportion of Delayed Flights by Month',\n             labels={'proportion_delayed': 'Proportion Delayed'},\n             category_orders={'month': ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']})\n\n# Show the plot\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-4",
    "href": "Projects/project2.html#questiontask-4",
    "title": "Project 2: Late flights and missing data",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nAccording to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). You will need to replace all the missing values in the Late Aircraft variable with the mean. Show your work by printing the first 5 rows of data in a table. Use these three rules for your calculations:\n100% of delayed flights in the Weather category are due to weather\n30% of all delayed flights in the Late-Arriving category are due to weather.\nFrom April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.\nThe analysis shows how many flights at each airport were delayed due to weather, including both severe and mild conditions. The distribution of delays among the various categories is displayed in the table. Of all the airports under investigation, ORD had the greatest amount of weather-related delays (4502.25 flights). ATL had the second-highest number of delays (3769.43), followed by DEN (1119.15 flights), IAD (960.15 flights), and SAN (674.70 flights). These results highlight how weather affects flight delays significantly and how important it is to take into account both severe and mild weather when estimating total delays. \n\n\nRead and format data\n# Include and execute your code here\n\n# Calculate the mean of 'Late Aircraft' excluding missing values\nmean_delay_late_aircraft = df[df['num_of_delays_late_aircraft'] != -999]['num_of_delays_late_aircraft'].mean()\n\n# Replace missing values with the mean\ndf['num_of_delays_late_aircraft'] = df['num_of_delays_late_aircraft'].replace(-999, mean_delay_late_aircraft)\n\ndf['severe'] = df['num_of_delays_weather']\ndf['mild_late'] = df['num_of_delays_late_aircraft'] * 0.3\ndf['mild_nas'] = np.where(df['month'].isin(['April', 'May', 'June', 'July', 'August']),\n                          df['num_of_delays_nas'] * 0.4,\n                          df['num_of_delays_nas'] * 0.65)\ndf['weather_total'] = df['severe'] + df['mild_late'] + df['mild_nas']\n\n# Check and convert data types before rounding\nnumeric_columns = ['severe', 'mild_late', 'mild_nas', 'weather_total']\ndf[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n\n# Format and rounding only numeric columns\ndf[numeric_columns] = df[numeric_columns].apply(lambda x: round(x,2))\n\n# Display the first 5 rows of the DataFrame with selected columns\nprint(df[numeric_columns].head())\n\n\n   severe  mild_late  mild_nas  weather_total\n0     448     332.73   2988.70        3769.43\n1     233     278.40    607.75        1119.15\n2      61     317.40    581.75         960.15\n3     306     676.50   3519.75        4502.25\n4      56     204.00    414.70         674.70",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-5",
    "href": "Projects/project2.html#questiontask-5",
    "title": "Project 2: Late flights and missing data",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.\nThe figure gives important information about how weather affects flight operations by showing the percentage of flights delayed at each airport. According to the analysis, SFO has the highest percentage of weather-related flight delays (9.7% of all flights). ORD closely follows with 8.6% of flights experiencing weather delays. DEN and IAD exhibit similar impacts. On the other hand, SLC demonstrates the lowest proportion of flights delayed by weather at 4.3%.These results highlight how different airports experience different degrees of weather-related delays, highlighting how important weather is in influencing flight performance. (To learn more, hover your cursor over the bars.)\n\n\nRead and format data\n# Group by airport_code and calculate the total number of flights and weather-related delays\n\nweather_summary = df.groupby('airport_code')[['num_of_flights_total', 'weather_total']].sum().reset_index()\n\n# Calculate the proportion of flights delayed by weather\nweather_summary['proportion_delayed_weather'] = weather_summary['weather_total'] / weather_summary['num_of_flights_total']\n\n# Sort the data for better visualization\nweather_summary = weather_summary.sort_values('proportion_delayed_weather', ascending=False)\n\n# Create a bar plot using Plotly Express\nfig = px.bar(weather_summary,\n             x='airport_code',\n             y='proportion_delayed_weather',\n             title='Proportion of Flights Delayed by Weather at Each Airport',\n             labels={'proportion_delayed_weather': 'Proportion of Flights Delayed by Weather'},\n             color='proportion_delayed_weather',\n             color_continuous_scale='blues')\n\n# Show the figure\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "projects.html#repo-for-all-my-projects",
    "href": "projects.html#repo-for-all-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  }
]